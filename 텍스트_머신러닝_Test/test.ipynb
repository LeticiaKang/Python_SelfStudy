{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 형태소 분석/텍스트 클리닝\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "from nltk import word_tokenize\n",
    "import pandas as pd \n",
    "from tqdm import tqdm\n",
    "from konlpy.tag import Okt\n",
    "import re\n",
    "\n",
    "# 단어 추가를 위함\n",
    "# from ckonlpy.tag import Twitter\n",
    "\n",
    "# Soynlp 패키지 \n",
    "from soynlp.noun import LRNounExtractor\n",
    "from soynlp.word import WordExtractor\n",
    "from soynlp.tokenizer import LTokenizer\n",
    "# import urllib.request\n",
    "# from soynlp import DoublespaceLineCorpus\n",
    "# from soynlp.word import WordExtractor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('기업데이터(30)_(전처리).csv').iloc[:, 1:]\n",
    "df.columns = [\"com\", \"date\", \"duty\", \"status\", \"star\", \"summary\", \"good\", \"bad\", \"expect\", \"doc\"]\n",
    "df.head()\n",
    "df1 = df[[\"com\", \"date\", \"duty\", \"doc\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 불용어, 숫자, 띄어쓰기, 특수문자, 의성어, 이모티콘 Cleaning\n",
    "- doc -> processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_21064\\3087591807.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df1[\"processing\"] = clean_text(df1[\"doc\"])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def clean_text(texts):\n",
    "    #이모티콘 제거\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                            \"]+\", flags=re.UNICODE)\n",
    "\n",
    "    #분석에 어긋나는 불용어구 제외 (특수문자, 의성어)\n",
    "    han = re.compile(r'[ㄱ-ㅎㅏ-ㅣ!?~,\".\\n\\r#\\ufeff\\u200d]')\n",
    "    \n",
    "    corpus = []\n",
    "    for i in range(0, len(texts)):\n",
    "        review = re.sub(r'[@%\\\\*=()/~#&\\+á?\\xc3\\xa1\\-\\|\\.\\:\\;\\!\\-\\,\\_\\~\\$\\'\\\"]', '',str(texts[i])) #remove punctuation\n",
    "        review = re.sub(r'\\d+','', str(texts[i]))# remove number\n",
    "        review = review.lower() #lower case\n",
    "        review = re.sub(r'\\s+', ' ', review) #remove extra space\n",
    "        # review = re.sub(r'<[^>]+>','',review) #remove Html tags\n",
    "        review = re.sub(r'\\s+', ' ', review) #remove spaces\n",
    "        review = re.sub(r\"^\\s+\", '', review) #remove space from start\n",
    "        review = re.sub(r'\\s+$', '', review) #remove space from the end\n",
    "        review = re.sub(han, '', review) #remove 특수문자, 의성어\n",
    "        review = re.sub(emoji_pattern, '', review) #remove 이모티콘\n",
    "        corpus.append(review)\n",
    "    return corpus\n",
    "\n",
    "df1[\"processing\"] = clean_text(df1[\"doc\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### soynlp 사용\n",
    ": 기존의 형태소 분석기는 신조어나 형태소 분석기에 등록되지 않은 단어 같은 경우에는 제대로 구분하지 못하는 단점"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting soynlp\n",
      "  Downloading soynlp-0.0.493-py3-none-any.whl (416 kB)\n",
      "     ------------------------------------- 416.8/416.8 kB 13.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from soynlp) (1.0.2)\n",
      "Requirement already satisfied: numpy>=1.12.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from soynlp) (1.23.4)\n",
      "Requirement already satisfied: psutil>=5.0.1 in c:\\users\\user\\appdata\\roaming\\python\\python38\\site-packages (from soynlp) (5.9.4)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from soynlp) (1.4.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn>=0.20.0->soynlp) (2.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn>=0.20.0->soynlp) (1.1.0)\n",
      "Installing collected packages: soynlp\n",
      "Successfully installed soynlp-0.0.493\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install soynlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45400"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = df['doc'][:45400]\n",
    "doc.to_csv('training_file.txt', index=False, header=None, sep=\"\\t\")\n",
    "#index = False : 자동으로 가장 왼쪽 컬럼에 생성된 0 부터 시작하는 인덱스 지울 때\n",
    "# header = None : 헤더 이름 지울 때\n",
    "# sep = \"\\t\" : CSV 파일 기본이 comma 라서, 별도의 구분자를 두려면 변경. 예제는 탭(\\t) 으로 바꿔 줌"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " soynlp는 학습 기반의 단어 토크나이저이므로 기존의 KoNLPy에서 제공하는 형태소 분석기들과는 달리 학습 과정을 거쳐야 합니다. 이는 전체 코퍼스로부터 응집 확률과 브랜칭 엔트로피 단어 점수표를 만드는 과정입니다. WordExtractor.extract()를 통해서 전체 코퍼스에 대해 단어 점수표를 계산합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45400"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainSet = DoublespaceLineCorpus(\"training_file.txt\")\n",
    "len(trainSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 0.983 Gbory 0.891 Gb\n",
      "all cohesion probabilities was computed. # words = 106076\n",
      "all branching entropies was computed # words = 133761\n",
      "all accessor variety was computed # words = 133761\n"
     ]
    }
   ],
   "source": [
    "word_extractor = WordExtractor()\n",
    "word_extractor.train(trainSet)\n",
    "word_score_table = word_extractor.extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. SOYNLP의 응집 확률(cohesion probability)\n",
    "- 응집 확률은 내부 문자열(substring)이 얼마나 응집하여 자주 등장하는지를 판단하는 척도입니다. 응집 확률은 문자열을 문자 단위로 분리하여 내부 문자열을 만드는 과정에서 왼쪽부터 순서대로 문자를 추가하면서 각 문자열이 주어졌을 때 그 다음 문자가 나올 확률을 계산하여 누적곱을 한 값입니다. 이 값이 높을수록 전체 코퍼스에서 이 문자열 시퀀스는 하나의 단어로 등장할 가능성이 높습니다. 수식은 아래와 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8509324758842444\n",
      "0.6642937404743782\n",
      "0.2569365176291876\n"
     ]
    }
   ],
   "source": [
    "print(word_score_table[\"워라\"].cohesion_forward)\n",
    "print(word_score_table[\"워라밸\"].cohesion_forward)\n",
    "print(word_score_table[\"네카라쿠배\"].cohesion_forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. SOYNLP의 브랜칭 엔트로피(branching entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.700857074091836\n",
      "3.247554193256744\n",
      "-0.0\n",
      "-0.0\n",
      "2.6031790172675286\n",
      "3.1256296092239726\n"
     ]
    }
   ],
   "source": [
    "print(word_score_table[\"워라\"].right_branching_entropy)\n",
    "print(word_score_table[\"워라밸\"].right_branching_entropy)\n",
    "print(word_score_table[\"네카라\"].right_branching_entropy)\n",
    "print(word_score_table[\"네카라쿠배\"].right_branching_entropy)\n",
    "print(word_score_table[\"주말근무\"].right_branching_entropy)\n",
    "print(word_score_table[\"네임벨류\"].right_branching_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. SOYNLP의 tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] used default noun predictor; Sejong corpus predictor\n",
      "[Noun Extractor] used noun_predictor_sejong\n",
      "[Noun Extractor] All 2398 r features was loaded\n",
      "[Noun Extractor] scanning was done (L,R) has (70019, 39778) tokens\n",
      "[Noun Extractor] building L-R graph was done\n",
      "[Noun Extractor] 13613 nouns are extracted\n",
      "training was done. used memory 1.294 Gbory 1.135 Gb\n",
      "all cohesion probabilities was computed. # words = 16330\n",
      "all branching entropies was computed # words = 130545\n",
      "all accessor variety was computed # words = 130545\n"
     ]
    }
   ],
   "source": [
    "noun_extractor = LRNounExtractor()\n",
    "nouns = noun_extractor.train_extract(list(trainSet)) # list of str like\n",
    "\n",
    "\n",
    "word_extractor = WordExtractor(\n",
    "    min_frequency=50, # example\n",
    "    min_cohesion_forward=0.05,\n",
    "    min_right_branching_entropy=0.0\n",
    ")\n",
    "\n",
    "word_extractor.train(list(trainSet))\n",
    "words = word_extractor.extract()\n",
    "\n",
    "cohesion_score = {word:score.cohesion_forward for word, score in words.items()}\n",
    "\n",
    "noun_scores = {noun:score.score for noun, score in nouns.items()}\n",
    "combined_scores = {noun:score + cohesion_score.get(noun, 0)\n",
    "    for noun, score in noun_scores.items()}\n",
    "combined_scores.update(\n",
    "    {subword:cohesion for subword, cohesion in cohesion_score.items()\n",
    "    if not (subword in combined_scores)}\n",
    ")\n",
    "\n",
    "tokenizer = LTokenizer(scores=combined_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "좋은 인력풀에서 일해볼 수 있는 기회여서 시야를 틀수도 있지만 부바부 전체적으로 자유로운 분위기 이지만 이또한 부바부로 생긴지 오래되지 않은 부서여서 매우 자유로운편이었음 실력일는 팀원이 아닌 리더에게 잘보이는 사람에게 과업을 몰아주는 정치질도 있음. 물론 인센티브도 같이 뺏김 오래다니지 않아서 경영진까지 바라고 할게 없었다고 한다\n",
      "['좋은', '인력', '풀에서', '일해볼', '수', '있는', '기회', '여서', '시야', '를', '틀수도', '있지', '만', '부바', '부', '전체적', '으로', '자유', '로운', '분위기', '이지', '만', '이또', '한', '부바', '부로', '생긴지', '오래', '되지', '않은', '부서', '여서', '매우', '자유', '로운편이었음', '실력', '일는', '팀원', '이', '아닌', '리더', '에게', '잘보', '이는', '사람', '에게', '과업', '을', '몰아주', '는', '정치', '질도', '있음.', '물론', '인센티브', '도', '같이', '뺏김', '오래', '다니지', '않아서', '경영진', '까지', '바라', '고', '할게', '없었다', '고', '한다']\n"
     ]
    }
   ],
   "source": [
    "train_list=list(df['doc'])\n",
    "print(str(train_list[34]))\n",
    "print(tokenizer.tokenize(str(train_list[34])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>com</th>\n",
       "      <th>date</th>\n",
       "      <th>duty</th>\n",
       "      <th>doc</th>\n",
       "      <th>soy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>네이버</td>\n",
       "      <td>2022. 11</td>\n",
       "      <td>디자인</td>\n",
       "      <td>커리어 경력 쌓고 싶은 사람에게 추천 수평적 사무실 분위기와 복지가 다른 곳 보다는...</td>\n",
       "      <td>[커리어, 경력, 쌓고, 싶은, 사람, 에게, 추천, 수평적, 사무실, 분위기, 와...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>네이버</td>\n",
       "      <td>2022. 11</td>\n",
       "      <td>전문직</td>\n",
       "      <td>자유로운 복장 분위기가 일단 편해서 좋았어요. 물론 업무는 당연히 강도가 있어야 할...</td>\n",
       "      <td>[자유, 로운, 복장, 분위기, 가, 일단, 편해서, 좋았어요, ., 물론, 업무,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>네이버</td>\n",
       "      <td>2022. 11</td>\n",
       "      <td>IT/인터넷</td>\n",
       "      <td>워라밸과 성장을 동시에 챙길 수 있는 몇 안되는 기업 앞으로도 가장 전망이 좋은 플...</td>\n",
       "      <td>[워라밸, 과, 성장, 을, 동시, 에, 챙길, 수, 있는, 몇, 안되는, 기업, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>네이버</td>\n",
       "      <td>2022. 11</td>\n",
       "      <td>IT/인터넷</td>\n",
       "      <td>개발자가 영향력을 좀 발휘하며 일할수 있는곳. it업계 1위 개발에만 집중하며 일할...</td>\n",
       "      <td>[개발자, 가, 영향력, 을, 좀, 발휘, 하며, 일할수, 있는곳, ., it, 업...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>네이버</td>\n",
       "      <td>2022. 10</td>\n",
       "      <td>IT/인터넷</td>\n",
       "      <td>아르바이트 생이었지만 꿈의 직장이란게 이런 것이구나 느낄 수 있었다 휴가 연차 등 ...</td>\n",
       "      <td>[아르바이트, 생이었지만, 꿈의, 직장, 이란게, 이런, 것이, 구나, 느낄, 수,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   com      date    duty                                                doc  \\\n",
       "0  네이버  2022. 11     디자인  커리어 경력 쌓고 싶은 사람에게 추천 수평적 사무실 분위기와 복지가 다른 곳 보다는...   \n",
       "1  네이버  2022. 11     전문직  자유로운 복장 분위기가 일단 편해서 좋았어요. 물론 업무는 당연히 강도가 있어야 할...   \n",
       "2  네이버  2022. 11  IT/인터넷  워라밸과 성장을 동시에 챙길 수 있는 몇 안되는 기업 앞으로도 가장 전망이 좋은 플...   \n",
       "3  네이버  2022. 11  IT/인터넷  개발자가 영향력을 좀 발휘하며 일할수 있는곳. it업계 1위 개발에만 집중하며 일할...   \n",
       "4  네이버  2022. 10  IT/인터넷  아르바이트 생이었지만 꿈의 직장이란게 이런 것이구나 느낄 수 있었다 휴가 연차 등 ...   \n",
       "\n",
       "                                                 soy  \n",
       "0  [커리어, 경력, 쌓고, 싶은, 사람, 에게, 추천, 수평적, 사무실, 분위기, 와...  \n",
       "1  [자유, 로운, 복장, 분위기, 가, 일단, 편해서, 좋았어요, ., 물론, 업무,...  \n",
       "2  [워라밸, 과, 성장, 을, 동시, 에, 챙길, 수, 있는, 몇, 안되는, 기업, ...  \n",
       "3  [개발자, 가, 영향력, 을, 좀, 발휘, 하며, 일할수, 있는곳, ., it, 업...  \n",
       "4  [아르바이트, 생이었지만, 꿈의, 직장, 이란게, 이런, 것이, 구나, 느낄, 수,...  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 빈 컬럼 만들기\n",
    "df_sonlp = df1[['com', 'date', 'duty', 'doc']]\n",
    "df_sonlp['soy'] = \"\"\n",
    "# df_sonlp['soy']\n",
    "\n",
    "# 빈 컬럼에 토큰화된 결과 추가하기\n",
    "for num in range(len(df_sonlp)):\n",
    "    df_sonlp['soy'][num] = tokenizer.tokenize(str(train_list[num]))\n",
    "    \n",
    "# 결과 출력\n",
    "df_sonlp.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60573/60573 [00:00<00:00, 171296.44it/s]\n"
     ]
    }
   ],
   "source": [
    "# 불용어 사전\n",
    "stopwords = list(pd.read_csv(\"stopwords_kej.csv\")['words'])\n",
    "stopwords.extend(\"이\")\n",
    "stopwords = set(stopwords)\n",
    "\n",
    "# 불용어 제거 함수\n",
    "def DeleteStopwords(cell):\n",
    "    words = [word for word in cell if word not in stopwords]\n",
    "    return words    \n",
    "\n",
    "# 형태소 분석 적용\n",
    "tokenizing_doc = []\n",
    "for cell in tqdm(df_sonlp['soy']):\n",
    "    tokenizing_doc.append(DeleteStopwords(cell))\n",
    "df_sonlp['cleaning_soy'] = tokenizing_doc\n",
    "\n",
    "df_sonlp.to_csv('불용어처리 결과.csv', encoding='utf-8-sig', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  words\n",
      "0    있는\n",
      "1    소생\n"
     ]
    }
   ],
   "source": [
    "# # ★★★★ stopwords 업데이트 꼭 하기~ ★★★★\n",
    "# save_stopwords = pd.DataFrame(stopwords, columns=[\"words\"])\n",
    "# print(save_stopwords.head(2))\n",
    "# save_stopwords.to_csv(\"stopwords_kej.csv\", encoding='utf-8-sig', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a077222d77dfe082b8f1dd562ad70e458ac2ab76993a0b248ab0476e32e9e8dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
